#
# import torch.fft
#
#
# # --- Attention –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ ---
# class FrequencyAttention(nn.Module):
#     def __init__(self, embed_dim):
#         super().__init__()
#         self.Wq = nn.Linear(embed_dim, embed_dim)
#         self.Wk = nn.Linear(embed_dim, embed_dim)
#         self.Wv = nn.Linear(embed_dim, embed_dim)
#         self.softmax = nn.Softmax(dim=-1)
#
#     def forward(self, noise, text_emb):
#         noise_fft = torch.fft.fft(noise, dim=-1)  # FFT –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
#         Q = self.Wq(noise_fft.real)  # Query = —Ä–µ–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã —à—É–º–∞
#         K = self.Wk(text_emb)  # Key = —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
#         V = self.Wv(noise_fft.real)  # Value = —á–∞—Å—Ç–æ—Ç—ã —à—É–º–∞
#
#         attention_scores = self.softmax(Q @ K.T / torch.sqrt(torch.tensor(K.shape[-1], dtype=torch.float32)))
#         modified_freqs = attention_scores @ V  # –ú–µ–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç—ã —à—É–º–∞
#
#         # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ñ–∞–∑—É –Ω–µ–∏–∑–º–µ–Ω–Ω–æ–π)
#         modified_noise_fft = torch.complex(modified_freqs, noise_fft.imag)
#         adapted_noise = torch.fft.ifft(modified_noise_fft, dim=-1).real
#
#         return adapted_noise
#
#
# # --- –í—Å—Ç—Ä–∞–∏–≤–∞–µ–º FFT-Attention –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–∫—É ---
# class FFTDiffusionModel(nn.Module):
#     def __init__(self, img_size, embed_dim):
#         super().__init__()
#         self.attn = FrequencyAttention(embed_dim)
#         self.unet = SimpleUNet(img_size)
#
#     def forward(self, xt, text_emb):
#         adapted_noise = self.attn(xt, text_emb)
#         xt_attn = xt + adapted_noise  # –î–æ–±–∞–≤–ª—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —à—É–º
#         predicted_noise = self.unet(xt_attn)  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —à—É–º —á–µ—Ä–µ–∑ U-Net
#         return predicted_noise
#
#
# # --- –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---
# fft_model = FFTDiffusionModel(IMG_SIZE, 256).to(device)
# optimizer_fft = optim.Adam(fft_model.parameters(), lr=LR)
#
#
# # --- –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è ---
# def train_fft_ddpm(model, dataset, epochs=10):
#     model.train()
#     for epoch in range(epochs):
#         for x0, text_emb in dataset:
#             x0, text_emb = x0.to(device), text_emb.to(device)
#             t = torch.randint(0, T, (BATCH_SIZE,), device=device)
#             xt = forward_diffusion(x0, t, alphas_bar)
#
#             predicted_noise = model(xt, text_emb)
#             loss = criterion(predicted_noise, torch.randn_like(xt))
#
#             optimizer_fft.zero_grad()
#             loss.backward()
#             optimizer_fft.step()
#
#         print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")
#
#
# print("FFT-Attention Diffusion Model –≥–æ—Ç–æ–≤–∞! ‚úÖ")
#
#
# # --- –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, COCO-Text) ---
# train_dataset = DataLoader(COCOTextDataset(), batch_size=BATCH_SIZE, shuffle=True)
#
# # --- –û–±—É—á–∞–µ–º –æ–±–µ –º–æ–¥–µ–ª–∏ ---
# print("–û–±—É—á–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π DDPM...")
# train_ddpm(model, train_dataset, epochs=10)
#
# print("–û–±—É—á–∞–µ–º FFT-Attention Diffusion...")
# train_fft_ddpm(fft_model, train_dataset, epochs=10)
#
# # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ ---
# torch.save(model.state_dict(), "ddpm_model.pth")
# torch.save(fft_model.state_dict(), "fft_attention_model.pth")
#
# print("–ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã! üöÄ")










# 12.03.2025
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
#
# # --- –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---
# IMG_SIZE = 32  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
# T = 1000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–∏
# BATCH_SIZE = 32
# LR = 1e-4
#
#
# # --- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–≤–∞—Ä–¥–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ (–∑–∞—à—É–º–ª–µ–Ω–∏–µ) ---
# def forward_diffusion(x0, t, alphas_bar, noise=None):
#     """ –î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é """
#     if noise is None:
#         noise = torch.randn_like(x0)
#     xt = torch.sqrt(alphas_bar[t]) * x0 + torch.sqrt(1 - alphas_bar[t]) * noise
#     return xt
#
#
# class SinusoidalTimeEmbedding(nn.Module):
#     def __init__(self, embed_dim):
#         super().__init__()
#         self.embed_dim = embed_dim
#
#     def forward(self, t):
#         """t ‚Äî —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ [0, T], —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (B,)"""
#         half_dim = self.embed_dim // 2
#         freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32) * (torch.log(torch.tensor(10000.0)) / half_dim))
#         angles = t[:, None] * freqs[None, :]
#         time_embedding = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)
#         return time_embedding  # (B, embed_dim)
#
#
# class CrossAttention(nn.Module):
#     def __init__(self, channels, num_heads=8):
#         super().__init__()
#         self.num_heads = num_heads
#         self.scale = (channels // num_heads) ** -0.5
#         self.Wq = nn.Linear(channels, channels)  # Query –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
#         self.Wk = nn.Linear(channels, channels)  # Key –¥–ª—è —Ç–µ–∫—Å—Ç–∞
#         self.Wv = nn.Linear(channels, channels)  # Value –¥–ª—è —Ç–µ–∫—Å—Ç–∞
#
#         self.softmax = nn.Softmax(dim=-1)
#
#     def forward(self, image_features, text_features):
#         Q = self.Wq(image_features)  # (B, H*W, channels)
#         K = self.Wk(text_features)  # (B, T, channels)
#         V = self.Wv(text_features)  # (B, T, channels)
#
#         attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, H*W, T)
#         attn_weights = self.softmax(attn_scores)  # (B, H*W, T)
#
#         output = torch.matmul(attn_weights, V)  # (B, H*W, channels)
#         return output
#
#
# class DeepBottleneck(nn.Module):
#     def __init__(self, text_emb_dim):
#         super().__init__()
#
#         self.deep_block_1 = UNetBlock(256, 256, 256)
#         self.deep_block_2 = UNetBlock(256, 256, 256)
#         self.deep_block_3 = UNetBlock(256, 256, 256)
#
#         # self.deep_block_1 = DeepBottleneckBlock(256, 256, 256)
#         # self.deep_block_2 = DeepBottleneckBlock(256, 256, 256)
#         # self.deep_block_3 = DeepBottleneckBlock(256, 256, 256)
#         # self.conv1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
#         # self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
#         # self.conv3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
#         self.proj_text = nn.Linear(text_emb_dim, 256)  # –ü—Ä–∏–≤–æ–¥–∏–º CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥ –∫ C
#
#         self.cross_attn = CrossAttention(256)  # Cross Attention –Ω–∞ —É—Ä–æ–≤–Ω–µ bottleneck
#         self.residual = nn.Conv2d(256, 256,
#                                   kernel_size=1)  # Residual Block (–¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–Ω–∞–ª–æ–≤, –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–ª–æ–π —Å–≤—ë—Ä—Ç–∫–∏)
#
#     def forward(self, x, text_emb, time_emb):
#         res = x  # –î–ª—è Residual Connection
#         x = self.deep_block_1(x, time_emb)
#         x = self.deep_block_2(x, time_emb)
#         x = self.deep_block_3(x, time_emb)
#         # x = F.relu(self.conv1(x))
#         # x = F.relu(self.conv2(x))
#         # x = F.relu(self.conv3(x))
#
#         B, C, H, W = x.shape
#         x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # (B, H*W, C)
#         proj_text_emb = text_emb
#         if text_emb.size(2) != x_flat.size(2):
#             proj_text_emb = self.proj_text(text_emb)
#         x_attn = self.cross_attn(x_flat, proj_text_emb)  # –ü—Ä–∏–º–µ–Ω—è–µ–º Attention
#         x_attn = x_attn.permute(0, 2, 1).view(B, C, H, W)  # (B, C, H, W)
#
#         # x = apply_cross_attention(x, self.cross_attn, self.proj_text, text_emb)
#         # x = self.cross_attn(x, text_emb)  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
#         x = F.relu(x_attn)  # ???
#         x = x + self.residual(res)  # Residual Skip Connection
#         return x
#
#
# # class DeepBottleneckBlock(nn.Module):
# #     def __init__(self, in_channels, out_channels, time_emb_dim):
# #         super().__init__()
# #         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
# #         self.time_mlp = nn.Linear(time_emb_dim, out_channels)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º `t`
# #         self.relu = nn.ReLU()
# #
# #     def forward(self, x, time_emb):
# #         t_emb = self.time_mlp(time_emb)[:, :, None, None]  # (B, C) -> (B, C, 1, 1)
# #         x = self.conv(x) + t_emb  # –î–æ–±–∞–≤–ª—è–µ–º `t` –∫ —Ñ–∏—á–∞–º
# #         return self.relu(x)
#
#
# class UNetBlock(nn.Module):
#     def __init__(self, in_channels, out_channels, time_emb_dim):
#         super().__init__()
#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
#         self.time_mlp = nn.Linear(time_emb_dim, out_channels)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º `t`
#         self.relu = nn.ReLU()
#
#     # x=(B, inC, H, W)->x=(B, outC, H, W); time_emb=(B, te)->time_emb=(B, outC, 1, 1) (te –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–µ–Ω time_emb_dim)
#     def forward(self, x, time_emb):
#         t_emb = self.time_mlp(time_emb)[:, :, None, None]  # (B, outC) -> (B, outC, 1, 1)
#         x = self.conv(x) + t_emb  # –î–æ–±–∞–≤–ª—è–µ–º `t` –∫ —Ñ–∏—á–∞–º
#         return self.relu(x)
#
#
# class UNet(nn.Module):
#     def __init__(self, TXT_EMB_DIM):
#         super().__init__()
#
#         self.time_embedding = SinusoidalTimeEmbedding(
#             256)  # (—ç—Ç–æ —á–∏—Å–ª–æ (256) –¥–æ–ª–∂–Ω–æ —Å–æ–æ—Ç–≤–µ—Å—Ç–≤–æ–≤–∞—Ç—å 3-–µ–º—É –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —á–∏—Å–ª—É –≤ UNetBlock)
#         # --- Downsampling (–°–∂–∞—Ç–∏–µ) ---
#         self.unet_enc_1 = UNetBlock(3, 64, 256)
#         self.unet_enc_2 = UNetBlock(64, 128, 256)
#
#         # self.enc1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
#         # self.enc2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
#         self.pool = nn.MaxPool2d(2, 2)  # –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ 2 —Ä–∞–∑–∞
#
#         # --- Bottleneck (–°–∞–º–∞—è —É–∑–∫–∞—è —á–∞—Å—Ç—å) ---
#         self.deep_bottleneck = DeepBottleneck(TXT_EMB_DIM)
#         # self.bottleneck = nn.Conv2d(128, 256, kernel_size=3, padding=1)
#
#         # self.cross_attn_bottleneck = CrossAttention(embed_dim)
#         self.cross_attn_upsamling_1 = CrossAttention(128)
#         self.proj_text_up_1 = nn.Linear(TXT_EMB_DIM, 128)
#         self.cross_attn_upsamling_2 = CrossAttention(64)
#         self.proj_text_up_2 = nn.Linear(TXT_EMB_DIM, 64)
#
#         # --- Upsampling (–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ) ---
#         self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
#         self.dec1 = nn.Conv2d(128 + 128, 64, kernel_size=3, padding=1)  # Skip connection
#         self.up2 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)
#         self.dec2 = nn.Conv2d(64 + 64, 3, kernel_size=3, padding=1)  # Skip connection
#
#     def forward(self, x, text_emb, time_t):
#         # --- Encoder (Downsampling) ---
#         x1 = self.unet_enc_1(x, self.time_embedding(time_t))  # time_t = (B, t)
#         # x1 = F.relu(self.enc1(x))  # (B, 64, H, W)
#         x1_pooled = self.pool(x1)  # (B, 64, H/2, W/2)
#         x2 = self.unet_enc_2(x1_pooled, self.time_embedding(time_t))
#         # x2 = F.relu(self.enc2(x1_pooled))  # (B, 128, H/2, W/2)
#         x2_pooled = self.pool(x2)  # (B, 128, H/4, W/4)
#
#         # --- Bottleneck ---
#         bottleneck = self.deep_bottleneck(x2_pooled, text_emb)  # (B, 256, H/4, W/4)
#
#         # --- Decoder (Upsampling) ---
#         x_up1 = self.up1(bottleneck)  # (B, 128, H/2, W/2)
#
#         x = x_up1
#         B, C, H, W = x.shape
#         x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # (B, H*W, C)
#         proj_text_emb = text_emb
#         if text_emb.size(2) != x_flat.size(2):
#             proj_text_emb = self.proj_text_up_1(text_emb)
#         x_attn = self.cross_attn_upsamling_1(x_flat, proj_text_emb)  # –ü—Ä–∏–º–µ–Ω—è–µ–º Attention
#         x_attn = x_attn.permute(0, 2, 1).view(B, C, H, W)  # (B, C, H, W)
#         x_up1_attn = x_attn
#         x_concat1 = torch.cat([x_up1_attn, x2], dim=1)
#         x_dec1 = F.relu(self.dec1(x_concat1))
#
#         x_up2 = self.up2(x_dec1)  # (B, 64, H, W)
#
#         x = x_up2
#         B, C, H, W = x.shape
#         x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # (B, H*W, C)
#         proj_text_emb = text_emb
#         if text_emb.size(2) != x_flat.size(2):
#             proj_text_emb = self.proj_text_up_2(text_emb)
#         x_attn = self.cross_attn_upsamling_2(x_flat, proj_text_emb)  # –ü—Ä–∏–º–µ–Ω—è–µ–º Attention
#         x_attn = x_attn.permute(0, 2, 1).view(B, C, H, W)  # (B, C, H, W)
#         x_up2_attn = x_attn
#         x_concat2 = torch.cat([x_up2_attn, x1], dim=1)
#         x_dec2 = F.relu(self.dec2(x_concat2))
#
#         return x_dec2
#
#
# # --- –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---
# device = "cuda" if torch.cuda.is_available() else "cpu"
# model = UNet(IMG_SIZE).to(device)
# optimizer = optim.Adam(model.parameters(), lr=LR)
# criterion = nn.MSELoss()
#
#
# # --- –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è ---
# def train_ddpm(model, dataset, epochs=10):
#     beta = torch.linspace(0.0001, 0.02, T)  # –õ–∏–Ω–µ–π–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏–µ Œ≤_t
#     alpha = 1 - beta  # Œ±_t
#     alphas_bar = torch.cumprod(alpha, dim=0)  # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç Œ±ÃÑ_t
#     print(alphas_bar.shape)  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å [1000] (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ t —Å–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ)
#     model.train()
#     for epoch in range(epochs):
#         for x0, _ in dataset:
#             x0 = x0.to(device)
#             t = torch.randint(0, T, (BATCH_SIZE,), device=device)  # —Å–ª—É—á–∞–π–Ω—ã–µ —à–∞–≥–∏ t
#
#             xt = forward_diffusion(x0, t, alphas_bar)  # –¥–æ–±–∞–≤–ª—è–µ–º —à—É–º
#             predicted_noise = model(xt)  # –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —à—É–º
#             loss = criterion(predicted_noise, torch.randn_like(xt))  # —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ä–µ–∞–ª—å–Ω—ã–º —à—É–º–æ–º
#
#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()
#
#         print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")
#
# # print("–ë–∞–∑–æ–≤–∞—è DDPM –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! ‚úÖ")
